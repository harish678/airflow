YOUR_FERNET_KEY --> aQ0a+mUfQ+N+5Bm5rcOSgJuiOA4A9mYp3UN9SgbrTZs=
			   
docker run --name airflowserver -d -e AIRFLOW__CORE__FERNET_KEY=$YOUR_FERNET_KEY -p 8080:8080 puckel/docker-airflow webserver

Airflow is platform to programmatically author, schedule and monitor workflows

5 core compoonents:
    1. Web Server
    2. Scheduler
    3. Metadata database
    4. Worker
    5. Executor
6 in distributed mode
    6. Queue

4 key concepts:
    1. DAG - graph of operators and dependencies
    2. Operator - define the task and what they do
    3. Task - instance of operator
    4. Task Instance - Specific run of task: DAG + Task + Point in TIME
    5. workflow - Mix of all the above

DAG
===
DAG can not have any cycle.
Each node is a TASK
Each edge is a DEPENDENCY

Properties
----------
DAG's are defined in Python files placed in Airflow's DAG_FOLDER (def. ~/airflow/dags)
dag_id - unique id
description
start_date - when dag should start
schedule_interval - how often dag should run
dependend_on_past - based on previous dag run state
default_args - dict of variables used as constructor paramater when initializing operators

Operators
=========
Definition of a single task
Should be idempotent - operator should produce same result on every run
Task is created by instantiating an Operator class

Types of Operators
1. Action Operators
	1. BashOperator
	2. PythonOperator
	3. EmailOperator
	4. MySqlOperator, SqliteOperator, PostgreOperator ...
2. Transfer Operators
	1. PrestoToMySqlOperator
	2. SftpOperator
3. Sensor Operators

DAGBAG -- checks for the dag configurations/dag directory refresh for every interval set in the configuration file 
																			(worker_refresh_interval (def: 30 secs))
																			(dag_dir_list_interval (def: 300 secs))
																			
Scheduler
=========
schedule_interval should be given as CRON JOB (str) or date (datetime.timedelta)

Parallelism 
-----------
In configuration file
1) parallelism
2) max_active_runs_per_dag
3) dag_concurrency 

SubDAG
======
subdagoperator and subdag_factory

Hooks
=====
Hook is used as an interface to interact with external systems

XCOM
====
Share data between tasks using XCOM's methods (xcom_push, xcom_pull)
XCOM with most recent execution_date will be pulled out in first if they both have the same key

Branching
=========
Allow DAG to choose different branch based on the result of a specific task (BranchPythonOperator). Not recommended to use 'dependend_on_past'.

SLA
===
add to an operator using sla=timedelta(seconds=5) # time based on the SLA
formula: execution_date + 2 schedule intervals ahead + sla.time < utcnow() -> SLA missed
checks for past execution task for calculating SLA

Plugins
=======
Create using airflow.plugins_manager.AirflowPlugin

Extendable Components
---------------------
1) Operators 		(BaseOperator)
2) Sensors 			(BaseSensor)
3) Hooks 			(BaseHook)
4) Executors		(BaseExecutor)
5) Admin Views		(flask_admin.BaseView)
6) Blueprints		(flask.Blueprint)
7) Menu Link		(flask_admin.base.MenuLink)

Variables
=========
from airflow.models import Variable -> get, set methods